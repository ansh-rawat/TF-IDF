{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# TFIDF Implementation without using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "# Taking a simple corpus example for this implementation without any capital letters and puntuations as we need to \n",
    "# compare our results with scikit-learn's implementation and scikit-lean deals with such strings differently.\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### Your custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns all the unique words from the corpus in an ascending order.\n",
    "def fit(data):\n",
    "    \n",
    "    vocab={} # Empty dictionary which would contain all the unique words.\n",
    "    temp=[] \n",
    "    \n",
    "    for sen in data: # To iterate through each document in the corpus.\n",
    "        sen=sen.split()\n",
    "        #print(sen)\n",
    "        \n",
    "        for word in sen: # To through each word in a document.\n",
    "            #print(word)\n",
    "            if word not in temp:\n",
    "                temp.append(word) # Adding only the unique words in the list temp.\n",
    "    \n",
    "    temp=sorted(temp) # Sorting the words alphabetically.\n",
    "    \n",
    "    for i in range(len(temp)):\n",
    "        vocab[temp[i]]=i # Adding indices to the sorted unique words.\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])\n"
     ]
    }
   ],
   "source": [
    "vocab = fit(corpus)\n",
    "print(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the IDF values for all the unique words generated from the corpus.\n",
    "def calc_idf(data,bag):\n",
    "    \n",
    "    idf=[] # Empty list which would contain all the IDF values of the unique words respectively.\n",
    "    N=len(data) # Getting the numerator value for the IDF formula which is total number of documents in a corpus.\n",
    "    \n",
    "    for word in bag.keys(): # Iterating through the each word from the bag of words.\n",
    "        deno=0 # Initializing denominator 0 for each word in the bag of words.\n",
    "        #print(word)\n",
    "        \n",
    "        for sen in data: # Iterating through the each document in the corpus.\n",
    "            sen=sen.split()\n",
    "            #print(sen)\n",
    "            if word in sen: # Checking if the word is available in the particular document.\n",
    "                deno+=1\n",
    "        \n",
    "        ln=(N+1)/(deno+1) \n",
    "        ln=1 + math.log(ln) # Calculating IDF.\n",
    "        #print(idf)\n",
    "        idf.append(ln) # Adding each of the IDF value generated in the idf list. \n",
    "    \n",
    "    return idf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    }
   ],
   "source": [
    "idf = calc_idf(corpus,vocab)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function which would return the sparse matrix of the TF-IDF values of the documents in a corpus.\n",
    "def calc_transform(data,vocab):\n",
    "    \n",
    "    rows=[] # Empty list which would contain row values for the sparse matrix.\n",
    "    columns=[] # Empty list which would contain column values for the sparse matrix.\n",
    "    values=[] # Empty list which would contain TF-IDF values for the sparse matrix.\n",
    "    \n",
    "    for sen in range(len(data)): # Iterating through each document in the corpus.\n",
    "        temp=dict(Counter(data[sen].split())) # Converting each document into the dictionary with keys as word and values as \n",
    "                                              # occurence of each word in that document.\n",
    "        #print(temp)\n",
    "        \n",
    "        for key,value in temp.items(): # Iterating through keys and values in the dictionary created above. \n",
    "            col=vocab.get(key) # Retrieving index of the document word from the bag of words dictionary.\n",
    "            #print(sen,col,value)\n",
    "            deno=len(data[sen].split()) # Retrieving denominator for TF formula which total number words in a document.\n",
    "            #print(deno)\n",
    "            tf=value/deno # Calculating TF.\n",
    "            #print(tf)\n",
    "            #print(idf[col])\n",
    "            tfidf=tf*idf[col] # Calculating TF-IDF value by retrieving respective IDF values calculated above.\n",
    "            #print('tfidf',tfidf)\n",
    "            rows.append(sen) # Adding respective row values which are required for sparse matrix creation.\n",
    "            columns.append(col) # Adding respective columns values which are required for sparse matrix creation.\n",
    "            values.append(tfidf) # Adding respective TF-IDF values which are required for sparse matrix creation.\n",
    "    \n",
    "    #print(rows,columns,values)\n",
    "    mat=csr_matrix((values,(rows,columns)),shape=(len(data),len(vocab))) #Getting sparse matrix from parameters retrieved above.\n",
    "    #print(mat[0])\n",
    "    mat=normalize(mat) # Applying L2 Normalization on the sparse matrix.\n",
    "    #print(mat[0])\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "IDF: [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    }
   ],
   "source": [
    "transformed = calc_transform(corpus,vocab)\n",
    "print('TFIDF:',transformed[0].toarray())\n",
    "print('IDF:',idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### Using scikit-learn to compare my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word')\n",
    "\n",
    "vec.fit(corpus)\n",
    "feature_matrix_2 = vec.transform(corpus)\n",
    "print(feature_matrix_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation values and scikit-learn's values both are same."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
